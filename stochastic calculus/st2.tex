\documentclass[a4paper, 10pt]{article}    
\usepackage{geometry}       
\geometry{a4paper}
\geometry{margin=1in} 
\usepackage{paralist}
  \let\itemize\compactitem
  \let\enditemize\endcompactitem
  \let\enumerate\compactenum
  \let\endenumerate\endcompactenum
  \let\description\compactdesc
  \let\enddescription\endcompactdesc
  \pltopsep=\medskipamount
  \plitemsep=1pt
  \plparsep=1pt
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{bbm, bm}
\usepackage{amsmath, amssymb, amsthm, mathrsfs}
\usepackage{booktabs, tikz, array, eurosym}
\usepackage{float}
\renewcommand{\arraystretch}{1.4}
\newcolumntype{L}{>{\arraybackslash}m{10cm}}
\newcommand\indep{\protect\mathpalette{\protect\indeP}{\perp}}
\def\indeP#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\pagestyle{headings}
\newcommand{\boxwidth}{430pt}

\theoremstyle{definition}
\newtheorem{problem}{Problem}

\newtheoremstyle{hSol}
  {1.0pt}% Space above
  {1.0pt}% Space below
  {}% bodyfont
  {}% indent
  {\bfseries}% thm head font
  {.}% punctuation after thm head
  { }% Space after thm head
  {}% thm head spec

\theoremstyle{hSol}
\newtheorem*{solution}{Solution}


\title{\textbf{Stochastic Calculus Assignment II}}
\author{Ze Yang~~~~(zey@andrew.cmu.edu)}

\begin{document}
\maketitle



\noindent\rule{16cm}{0.4pt}
%///////////////////////////////////////////////////////////////////////
\begin{problem}
\end{problem}
\begin{proof} \textbf{(a)} Let $A_{\alpha}:=\{w: X(w)\leq \alpha\}$, $B_{\alpha}:=\{w: Y(w)\leq \alpha\}$. Then by definition, $\sigma(X) = \sigma(\{A_{\alpha}: \alpha \in \mathbb{R}\})$, i.e. the sigma algebra generated by all sets with form $A_{\alpha}$. Similarly $\sigma(Y) = \sigma(\{B_{\alpha}: \alpha \in \mathbb{R}\})$.\\
Further, define $E_1=\{\text{first toss is head}\}$, $E_2=\{\text{second toss is head}\}$. By definition of $X,Y$ we know that $X=\mathbbm{1}_{E_1}$, $Y=\mathbbm{1}_{E_2}$. Therefore:
\begin{equation}
	A_{\alpha} = \begin{cases}
	\emptyset & \alpha < 0\\
	E_1^{\complement} & 0\leq \alpha < 1\\
	\Omega & \alpha \geq 1
	\end{cases}
\end{equation}
Hence
\begin{equation}
	\begin{split}
		\sigma(X) &= \sigma(\{A_{\alpha}: \alpha \in \mathbb{R}\}) = \{\emptyset, E_1, E_1^{\complement}, \Omega\}\\
		&= \{\emptyset, \{HH,HT\}, \{TH, TT\}, \{HH,HT,TH,HH\}\}
	\end{split}
\end{equation}
Similarly for $Y$ we have:
\begin{equation}
	B_{\alpha} = \begin{cases}
	\emptyset & \alpha < 0\\
	E_2^{\complement} & 0\leq \alpha < 1\\
	\Omega & \alpha \geq 1
	\end{cases}
\end{equation}
Hence
\begin{equation}
	\begin{split}
		\sigma(Y) &= \sigma(\{B_{\alpha}: \alpha \in \mathbb{R}\}) = \{\emptyset, E_2, E_2^{\complement}, \Omega\}\\
		&= \{\emptyset, \{HH,TH\}, \{HT, TT\}, \{HH,HT,TH,HH\}\}
	\end{split}
\end{equation}
~\\
\textbf{(b)} By definition, $\emptyset$ and $\Omega$ is independent to any set $A$. Because $A\cap \emptyset = \emptyset, A\cap \Omega= \Omega$. Hence one must have
\begin{equation}
	\begin{split}
		&\mathbb{P}\left(A\cap \emptyset\right) = 0 = \mathbb{P}\left(\emptyset\right)\mathbb{P}\left(A\right) \\
		&\mathbb{P}\left(A\cap \Omega \right) = \mathbb{P}\left(A\right)  = \mathbb{P}\left(\Omega\right)\mathbb{P}\left(A\right)
	\end{split}
\end{equation}
$X\perp Y \iff \sigma(X)\perp \sigma(Y)$. Since $|\sigma(X)|=|\sigma(Y)|=4$, it suffices to check $4^2 =16 $ pairs of set independence. Using the property above ($\Omega$ and $\emptyset$ are independent to any set), it suffices to only check $2^2=4$ pairs: $(E_1, E_2)$, $(E_1^{\complement}, E_2)$, $(E_1, E_2^{\complement})$, $(E_1^{\complement}, E_2^{\complement})$. Under this probability measuare, we have $\mathbb{P}\left(E_1\right) = \frac{1}{12}+\frac{1}{6} = \frac{1}{4}$; $\mathbb{P}\left(E_2\right) = \frac{1}{12}+\frac{1}{4} = \frac{1}{3}$. So:
\begin{equation}
	\begin{split}
		&\mathbb{P}\left(E_1 \cap E_2\right) = \mathbb{P}\left(HH\right) = \frac{1}{12} = \frac{1}{4}\times \frac{1}{3} = \mathbb{P}\left(E_1\right)\mathbb{P}\left(E_2\right)\\
		&\mathbb{P}\left(E_1^{\complement} \cap E_2\right) = \mathbb{P}\left(TH\right) = \frac{1}{4} = \left(1-\frac{1}{4}\right)\times \frac{1}{3} = \mathbb{P}\left(E_1^{\complement}\right)\mathbb{P}\left(E_2\right)\\
		&\mathbb{P}\left(E_1 \cap E_2^{\complement}\right) = \mathbb{P}\left(HT\right) = \frac{1}{6} = \frac{1}{4}\times\left(1- \frac{1}{3}\right) = \mathbb{P}\left(E_1\right)\mathbb{P}\left(E_2^{\complement}\right)\\
		&\mathbb{P}\left(E_1^{\complement} \cap E_2^{\complement}\right) = \mathbb{P}\left(TT\right) = \frac{1}{2} = \left(1-\frac{1}{4}\right)\times \left(1-\frac{1}{3}\right) = \mathbb{P}\left(E_1^{\complement}\right)\mathbb{P}\left(E_2^{\complement}\right)
	\end{split}
\end{equation}
Therefore $\sigma(X)\perp \sigma(Y)$ $\Rightarrow X\perp Y$.\\
~\\
\textbf{(c)} We have
\begin{equation}
	\begin{split}
		\tilde{\mathbb{P}}\left(E_1\right) = \frac{1}{4};~~~~\tilde{\mathbb{P}}\left(E_2\right)=\frac{11}{24}
	\end{split}
\end{equation}
Clearly, $\tilde{\mathbb{P}}\left(E_1 \cap E_2\right) = \tilde{\mathbb{P}}\left(HH\right) = \frac{1}{12}\ne \tilde{\mathbb{P}}\left(E_1\right)\tilde{\mathbb{P}}\left(E_2\right) = \frac{11}{96}$. So $X,Y$ are not independent in this case.

\end{proof}

\noindent\rule{16cm}{0.4pt}
%///////////////////////////////////////////////////////////////////////
\begin{problem}
\end{problem}
\begin{proof} \textbf{(a)} First off, by the construction of the probability measure, we know that
\begin{equation}
	\mathbb{P}\left(C_1\right) = p^3,~~~\mathbb{P}\left(C_2\right) = 3p^2q,~~~\mathbb{P}\left(C_3\right) = 3pq^2,~~~\mathbb{P}\left(C_4\right) = q^3
\end{equation}
The problem has already implies that $\mathbb{E}\left[S_2\middle|S_3\right]$ take the form of $\sum_{i=1}^4 c_i \mathbbm{1}_{C_i}$. By definition of conditional expectation: $\forall A \in \sigma(S_3)$:
$$
\int_A \mathbb{E}\left[S_2\middle|S_3\right] d \mathbb{P} = \int_A \sum_{i=1}^4c_i \mathbbm{1}_{C_i} d \mathbb{P} = \int_A S_2 d \mathbb{P}~~(\dag)
$$
Clearly by their definitions, $C_i \in \sigma(S_3)$. So we can evaluate ($\dag$) for all $C_i$. For any $i\in\{1,2,3,4\}$:
\begin{equation}
	LHS = \int_{C_i}\sum_{i=1}^4c_i \mathbbm{1}_{C_i} d \mathbb{P} = c_i \mathbb{P}\left(C_i\right) = \mathbb{E}\left[S_2; C_i\right]
\end{equation}
Where we denote $\mathbb{E}\left[X; A\right]:=\int_A X d \mathbb{P}$. Also, we have:
\begin{equation}
	\begin{split}
	&S_2(HHH) = 16;~~~S_2(HHT) = 16;~~~S_2(HTH) = 4;~~~S_2(THH) =4;\\
	&S_2(HTT) = 4;~~~S_2(THT) = 4;~~~S_2(TTH) = 1;~~~S_2(TTT) =1;
\end{split}
\end{equation}

\begin{itemize}
	\item[$\cdot$] $i=1$: $C_1 = \{HHH\}$. Hence $c_1p^3 = 16p^3$.
	\item[$\cdot$] $i=2$: $C_2 = \{HHT, HTH, THH\}$. Hence $3p^2qc_2  = 16\cdot p^2q + 4\cdot 2p^2q$.
	\item[$\cdot$] $i=3$: $C_3 = \{HTT, THT, TTH\}$. Hence $3pq^2c_3 = 1\cdot pq^2 + 4\cdot 2pq^2$.
	\item[$\cdot$] $i=4$: $C_4 = \{TTT\}$. Hence $c_4q^3 = 1q^3$.
\end{itemize}
To sum up:
\begin{equation}
	\begin{cases}
	c_1 = 16 \\
	c_2 = \frac{1}{3}\cdot 16 + \frac{2}{3}\cdot 4 = 8\\
	c_3 = \frac{1}{3}\cdot 1 + \frac{2}{3}\cdot 4 = 3\\
	c_4 = 1
	\end{cases}
\end{equation}
\begin{equation}
	\mathbb{E}\left[S_2\middle|S_3\right](w) = 16 \mathbbm{1}_{C_1}(w) + 8 \mathbbm{1}_{C_2}(w) + 3 \mathbbm{1}_{C_3}(w) + \mathbbm{1}_{C_4}(w)
\end{equation}
~\\
\textbf{(b)} First off, clearly $p+q=1$, so we have:
\begin{equation}
	\begin{split}
	\mathbb{E}\left[S_2\right] &= \sum_{w \in \Omega} S_2(w) \mathbb{P}\left(w\right) \\
	&= 16 p^3 + 16p^2q + 4 \times 2p^2q + 4 \times 2pq^2 + pq^2 + q^3 \\
	&=16p^2(p+q) + 4 \times 2pq(p+q) + q^2(p+q)\\
	&= 16p^2 + 4\times 2pq + q^2 \\
	&= 16p^2 + 8pq + q^2
	\end{split}
\end{equation}
Secondly, consider the expectation of conditional expectation:
\begin{equation}
	\begin{split}
		\mathbb{E}\left[\mathbb{E}\left[S_2\middle|S_3\right]\right] &= \mathbb{E}\left[\sum_{i=1}^4c_i \mathbbm{1}_{C_i}\right] = \sum_{i=1}^4 c_i \mathbb{P}\left(C_i\right)\\
		&=16 \mathbb{P}\left(C_1\right)+8\mathbb{P}\left(C_2\right)+3\mathbb{P}\left(C_3\right)+\mathbb{P}\left(C_4\right)\\
		&= 16p^3 + 8\cdot 3p^2q + 3\cdot 3pq^2 + q^3\\
		&= 16p^3 + (16+8)p^2q + (8+1)pq^2 + q^3\\
		&=16p^2(p+q) + 8pq(p+q) + q^2(p+q)\\
		&=16p^2 + 8pq + q^2 = \mathbb{E}\left[S_2\right]
	\end{split}
\end{equation}
\end{proof}

\noindent\rule{16cm}{0.4pt}
%///////////////////////////////////////////////////////////////////////
\begin{problem}
\end{problem}
\begin{solution} Since $s<t$, we write $W_t = W_s + (W_t - W_s) = W_s + \Delta_{s,t}$. 
\begin{itemize}
	\item[$\cdot$] We let $\Delta_{s,t} := W_t - W_s \sim \mathcal{N}(0, t-s)$. It follows that $\mathbb{E}\left[\Delta_{s,t}\right]=0$; $\mathbb{E}\left[\Delta_{s,t}^2\right] = t-s$; $\mathbb{E}\left[\Delta_{s,t}^3\right]=0$.
	\item[$\cdot$] Moreover, by property of Brownian motion (independent increments): $\Delta_{s,t} \perp \mathcal{F}_s$.
	\item[$\cdot$] In addition, $W_s$ is $\mathcal{F}_s$-measurable.
\end{itemize}
Therefore, we have
\begin{equation}
	\begin{split}
		\mathbb{E}\left[W_t^3\middle|\mathcal{F}_s\right] &= \mathbb{E}\left[(W_s + \Delta_{s,t})^3\middle|\mathcal{F}_s\right] \\
		&= \mathbb{E}\left[W_s^3\middle|\mathcal{F}_s\right] +\mathbb{E}\left[3W_s^2\Delta_{s,t}\middle|\mathcal{F}_s\right]+ \mathbb{E}\left[3W_s\Delta_{s,t}^2\middle|\mathcal{F}_s\right]+ \mathbb{E}\left[\Delta_{s,t}^3\middle|\mathcal{F}_s\right] \\
		&= W_s^3 + 3W_s^2 \mathbb{E}\left[\Delta_{s,t}\right] + 3W_s \mathbb{E}\left[\Delta_{s,t}^2\right] + \mathbb{E}\left[\Delta_{s,t}^3\right] \\
		&= W_s^3 + 3W_s^2\times 0 + 3W_s(t-s) + 0\\
		&= W_s^3  + 3W_s(t-s)
	\end{split}
\end{equation}
\end{solution}

\noindent\rule{16cm}{0.4pt}
%///////////////////////////////////////////////////////////////////////
\begin{problem}
\end{problem}
\begin{solution} \textbf{(a)} 
\begin{equation}
	\begin{split}
		\mathbb{E}\left[(e^{x+Y}-K)^+\right] &= \int_{-\infty}^{+\infty} (e^{x+y}-K)^+ f_Y(y) dy \\
		&= \int_{\log K - x}^{+\infty} (e^{x+y}-K) \frac{1}{\sqrt{2\pi}} e^{-\frac{y^2}{2}} dy \\
		&= \int_{\log K - x}^{+\infty} \exp\left(\frac{-(y^2-2x-2y)}{2}\right)\frac{1}{\sqrt{2\pi}} dy - K\int_{\log K - x}^{+\infty} f_Y(y) dy\\
		&= I_1 - I_2
	\end{split}
\end{equation}
We deal with the two parts separately. $Y$ is standard normal, $I_2 = K \mathbb{P}\left(Y>\log K -x\right) = K \mathbb{P}\left(Y<x-\log K\right) = K N(x-\log K)$. \\
On the other hand
\begin{equation}
	\begin{split}
		I_1 &= \int_{\log K - x}^{+\infty} \exp\left(\frac{-(y^2-2y+1)+(2x+1)}{2}\right)\frac{1}{\sqrt{2\pi}} dy \\
		&= e^{x+\frac{1}{2}} \int_{\log K - x}^{+\infty} \exp\left(\frac{-(y-1)^2}{2}\right)\frac{1}{\sqrt{2\pi}} dy \\
		&= e^{x+\frac{1}{2}} \mathbb{P}\left(\tilde{Y}> \log K-x\right)
	\end{split}
\end{equation}
Where $\tilde{Y}\sim \mathcal{N}(1, 1)$. Let $Z$ be a standard normal, we have
\begin{equation}
	I_1 = e^{x+\frac{1}{2}} \mathbb{P}\left(\tilde{Y} > \log K-x\right) = e^{x+\frac{1}{2}} \mathbb{P}\left(Z > \log K-x-1\right) = e^{x+\frac{1}{2}}N(x+1-\log K)
\end{equation}
Hence
\begin{equation}
	\mathbb{E}\left[(e^{x+Y}-K)^+\right]  = I_1 -I_2 = e^{x+\frac{1}{2}}N(x+1-\log K) - K N(x-\log K)
\end{equation}
\textbf{(b)} Since $X\perp Y$, then clearly $Y \perp \sigma(X)$, and $X$ is $\sigma(X)$-measurable. We can use the lemma 4.12 (\textit{independence lemma}):
\begin{equation}
	\begin{split}
		\mathbb{E}\left[(e^{X+Y}-K)^+| \sigma(X)\right] &= \int_{\mathbb{R}} (e^{X+y}-K)^+ f_Y(y)dy \\
		&= \int_{\log K - X}^{+\infty} (e^{X+y}-K) \frac{1}{\sqrt{2\pi}} e^{-\frac{y^2}{2}} dy \\
		&= \int_{\log K - X}^{+\infty} \exp\left(\frac{-(y^2-2X-2y)}{2}\right)\frac{1}{\sqrt{2\pi}} dy - K\int_{\log K - X}^{+\infty} f_Y(y) dy\\
		&= \tilde{I_1} - \tilde{I_2}
	\end{split}
\end{equation}
Note that the integrals have exactly same form as \textbf{(a)}, except that $x$ is substituted by $X=X(w)$. So we have:
\begin{equation}
	\mathbb{E}\left[(e^{X+Y}-K)^+| \sigma(X)\right] = e^{X+\frac{1}{2}}N(X+1-\log K) - K N(X-\log K) 
\end{equation}
i.e. 
\begin{equation}
	\mathbb{E}\left[(e^{X+Y}-K)^+| \sigma(X)\right] (w) = e^{X(w)+\frac{1}{2}}N(X(w)+1-\log K) - K N(X(w)-\log K) 
\end{equation}
\end{solution}

\noindent\rule{16cm}{0.4pt}
%///////////////////////////////////////////////////////////////////////
\begin{problem}
\end{problem}
\begin{solution} We have, $\forall \lambda \in \mathbb{R}$, $s<t$:
\begin{equation}
	\begin{split}
	\mathbb{E}\left[M_t\middle|\mathcal{F}_s\right] &= \mathbb{E}\left[\exp(\lambda W_t - \alpha t)\middle|\mathcal{F}_s\right]\\
	&= e^{-\alpha t}\mathbb{E}\left[\exp(\lambda W_s + \lambda (W_t - W_s))\middle|\mathcal{F}_s\right] \\
	&= e^{-\alpha t}e^{\lambda W_s}\mathbb{E}\left[\lambda (W_t - W_s)\middle|\mathcal{F}_s\right]\\
	&\quad~(e^{\lambda W_s}~is~\mathcal{F}_s\text{\textit{-measurable, and take out what is known.}})\\
	&= e^{-\alpha t}e^{\lambda W_s}\mathbb{E}\left[\lambda (W_t - W_s)\right]~~(\dag)\\
	&\quad~(\text{\textit{use the fact that }}e^{\lambda (W_t-W_s)}~\perp~\mathcal{F}_s)\\
	\end{split}
\end{equation}
Let $\Delta_{s,t} := W_t-W_s$, we know $\Delta_{t,s}\sim \mathcal{N}(0, t-s)$; hence $\mathbb{E}\left[\lambda (W_t-W_s)\right]$ is exactly the value of moment generating function of $\Delta_{s,t}$ evaluated at $t=\lambda$. Therefore, by the table of distributions sheet:
$$
\mathbb{E}\left[\lambda (W_t-W_s)\right] = \exp\left(0 + \frac{1}{2}\lambda^2(t-s)\right)
$$
It follows that
\begin{equation}
	\begin{split}
		(\dag) = \mathbb{E}\left[M_t\middle|\mathcal{F}_s\right]  = \exp\left(\lambda W_s - \alpha t + \frac{1}{2}\lambda^2(t-s)\right)
	\end{split}
\end{equation}
$M_t$ a martingale $\iff$ $\mathbb{E}\left[M_t\middle|\mathcal{F}_s\right]=M_s=\exp(\lambda W_s- \alpha s)$, i.e.
\begin{equation}
	\exp\left(\lambda W_s - \alpha t + \frac{1}{2}\lambda^2(t-s)\right) = \exp(\lambda W_s- \alpha s)
\end{equation}
$\Rightarrow \frac{1}{2}\lambda^2(t-s) = \alpha (t-s) \Rightarrow$ $\alpha = \frac{1}{2}\lambda^2$.
\end{solution}



\end{document}